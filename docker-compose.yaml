# Cloudbees CI high availability for traditional platforms demo

services:
  operations-center:
    image: ${DOCKER_IMAGE_OC}
    platform: linux/amd64
    extra_hosts:
      - "${OC_URL}:${HAPROXY_IP}"
      - "${CLIENTS_URL}:${HAPROXY_IP}"
    #privileged: true
    #user: root
    healthcheck:
      test: curl -k --fail ${HTTP_PROTOCOL}://localhost:${HTTP_PORT}/whoAmI/api/json?tree=authenticated || exit 1
      interval: 3s
      timeout: 3s
      retries: 150
    environment:
      - JAVA_OPTS=${CJOC_JAVA_OPTS}
      - JENKINS_OPTS=${CJOC_JENKINS_OPTS}
      - CJOC_LOGIN_PW=${CJOC_LOGIN_PW}
      - CJOC_LOGIN_USER=${CJOC_LOGIN_USER}
      - OC_URL=${HTTP_PROTOCOL}://${OC_URL}
      - CJOC_CASC_DEFAULT_BUNDLE=${CJOC_CASC_DEFAULT_BUNDLE}
      - CONTROLLER_BUNDLE_NAME=${CONTROLLER_BUNDLE_NAME}
      - CASC_BUNDLE_PATH_CONTAINER=${CASC_BUNDLE_PATH_CONTAINER}
    container_name: operations-center
    volumes:
      - ${OC_PERSISTENCE}:/var/jenkins_home
      - ./casc/cjoc:${CASC_BUNDLE_PATH_CONTAINER}/cjoc
      - ./casc/controller:${CASC_BUNDLE_PATH_CONTAINER}/controller
    networks:
      demo-network:
        ipv4_address: ${OC_IP}
  # Sidecar bunlde-link.yaml init doesent work yet, oc  hostname can not be resolved?
  init-controller:
    image: curlimages/curl
    user: "1000:1000"
    extra_hosts:
      - "${OC_URL}:${HAPROXY_IP}"
    command:
      - /bin/sh
      - -c
      - |
        set -e
        set -x
        export SECRET_PATH_CJOC_TOKEN="/tmp/${SECRET_FILE_CJOC_TOKEN}"
        echo "Waiting for Token file $${SECRET_PATH_CJOC_TOKEN} to be generated..."
        while [ ! -f "$${SECRET_PATH_CJOC_TOKEN}" ]; do
            sleep 2
        done
        TOKEN="$$(cat $${SECRET_PATH_CJOC_TOKEN})"
        echo "Fetching bundle link..."

        # Download bundle link from operationscenter
        ## Capture HTTP status code, allow 4xx/5xx to pass curl so we can check code
        ### Network workaround: we need to download from operationscenter:8080 directly because HAProxy is not up and running yet in this compose stage.
        ### -s: Silent (no progress), -S: Show error (connection errors), -k: Insecure, -w: Output status code, -L: Follow redirects
        HTTP_STATUS=$$(curl -sSL -k -w "%{http_code}" \
            --user "${CJOC_LOGIN_USER}:$${TOKEN}" \
            -o "${JENKINS_HOME_CONTAINER}/bundle-link.yaml" \
            "${HTTP_PROTOCOL}://operations-center:${HTTP_PORT}/config-bundle-download-link?id=${CONTROLLER_BUNDLE_NAME}")

        echo "HTTP Status: $$HTTP_STATUS"

        # Check for allowed status codes: 200, 201, 302
        case "$$HTTP_STATUS" in
            200|201|302)
                echo "Success: Bundle link downloaded."
                # Network workaround: Replacement of https://cjoc.local/zip-bundle/ with http://operations-center:8080/zip-bundle/ required, 
                # because the external route https://cjoc.local can not be resolved to the IP of haproxy at the time of execution because haproxy is not up an running yet. (see docker-compose dependencies chain)
                sed -i 's|http.*/zip-bundle/|${HTTP_PROTOCOL}://operations-center:${HTTP_PORT}/zip-bundle/|' ${JENKINS_HOME_CONTAINER}/bundle-link.yaml
                cat ${JENKINS_HOME_CONTAINER}/bundle-link.yaml
                ;;
            *)
                echo "Error: Failed to fetch bundle link. HTTP Status: $$HTTP_STATUS"
                if [ -s "${JENKINS_HOME_CONTAINER}/bundle-link.yaml" ]; then
                    echo "Response content:"
                    cat "${JENKINS_HOME_CONTAINER}/bundle-link.yaml"
                fi
                exit 1
                ;;
        esac
    container_name: init-controller
    healthcheck:
      test: test -f ${JENKINS_HOME_CONTAINER}/bundle-link.yaml
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 2s
    depends_on:
      operations-center:
        condition: service_healthy
    volumes:
      - ${CONTROLLER_PERSISTENCE}:${JENKINS_HOME_CONTAINER}
      # We mount the entire OC_PERSISTENCE directory to /var/oc_home instead of just the single cjoc_token.txt file. 
      # Mounting a single non-existent file into a directory that is already a volume mount
      # triggers an OCI runtime mount overlay error on Docker for Mac (virtiofs).
      - ${OC_PERSISTENCE}/${SECRET_FILE_CJOC_TOKEN}:/tmp/${SECRET_FILE_CJOC_TOKEN}
    networks:
      demo-network:
        ipv4_address: ${INIT_CONTROLLER_IP}

  ha-client-controller-1:
    image: ${DOCKER_IMAGE_CLIENT_CONTROLLER}
    platform: linux/amd64
    extra_hosts:
      - "${OC_URL}:${HAPROXY_IP}"
    #privileged: true
    # user: root
    healthcheck:
      test: curl -k --fail ${HTTP_PROTOCOL}://localhost:${HTTP_PORT}/whoAmI/api/json?tree=authenticated || exit 1
      interval: 6s
      timeout: 3s
      retries: 150
    # Required JAVA and JENKINS Options for HA (active/active)
    # https://docs.cloudbees.com/docs/cloudbees-ci/latest/ha-install-guide/specific-ha-installation-traditional#_java_options
    # https://docs.cloudbees.com/docs/cloudbees-ci/latest/ha/specific-ha-installation-traditional#_jenkins_args
    environment:
      - JAVA_OPTS=${CONTROLLER_JAVA_OPTS}
      - JENKINS_OPTS=${CONTROLLER_JENKINS_OPTS}
      - CONTROLLER_URL=${HTTP_PROTOCOL}://${CLIENTS_URL}
    depends_on:
      init-controller:
        condition: service_completed_successfully
        required: true
    # entrypoint: ["sh", "-c", "while [ ! -f /var/jenkins_home/bundle-link.yaml ]; do echo 'wait for bundle-link.yaml' && sleep 1; done;"]

    container_name: ha-client-controller-1
    # Controller 1 is mapping the same underlying dir as controller 2. They share the same jenkins_home
    # Cache dir is different for each
    volumes:
      - ${CONTROLLER_PERSISTENCE}:/var/jenkins_home
      - ${CONTROLLER1_CACHES}:/var/cache/cloudbees-core-cm
    networks:
      demo-network:
        ipv4_address: ${CONTROLLER1_IP}
  ha-client-controller-2:
    image: ${DOCKER_IMAGE_CLIENT_CONTROLLER}
    platform: linux/amd64
    extra_hosts:
      - "${OC_URL}:${HAPROXY_IP}"
    #privileged: true
    # user: root
    healthcheck:
      test: curl -k --fail  ${HTTP_PROTOCOL}://localhost:${HTTP_PORT}/whoAmI/api/json?tree=authenticated || exit 1
      interval: 6s
      timeout: 3s
      retries: 150
    # Required JAVA and JENKINS Options for HA (active/active)
    # https://docs.cloudbees.com/docs/cloudbees-ci/latest/ha-install-guide/specific-ha-installation-traditional#_java_options
    # https://docs.cloudbees.com/docs/cloudbees-ci/latest/ha/specific-ha-installation-traditional#_jenkins_args
    environment:
      - JAVA_OPTS=${CONTROLLER_JAVA_OPTS}
      - JENKINS_OPTS=${CONTROLLER_JENKINS_OPTS}
      - CONTROLLER_URL=${HTTP_PROTOCOL}://${CLIENTS_URL}
    depends_on:
      ha-client-controller-1:
        condition: service_healthy
    container_name: ha-client-controller-2
    # Controller 2 is mapping the same underlying dir as controller 1. They share the same jenkins_home
    # Cache dir is different for each
    volumes:
      - ${CONTROLLER_PERSISTENCE}:/var/jenkins_home
      - ${CONTROLLER2_CACHES}:/var/cache/cloudbees-core-cm
    networks:
      demo-network:
        ipv4_address: ${CONTROLLER2_IP}
  agent:
    #build:
    #  context: .
    #  dockerfile: Dockerfile.agent
    image: ${DOCKER_IMAGE_JENKINS_SSH_AGENT}
    privileged: true
    user: root
    container_name: ssh-agent
    #    expose:
    #      - 22
    volumes:
      - ${AGENT_PERSISTENCE}:/home/jenkins/
    networks:
      demo-network:
        ipv4_address: ${AGENT_IP}
    environment:
      - JENKINS_AGENT_SSH_PUBKEY=${JENKINS_AGENT_SSH_PUBKEY}
  haproxy:
    image: ${DOCKER_IMAGE_HAPROXY}
    volumes:
      - ${HA_PROXY_CONFIG}:/usr/local/etc/haproxy/haproxy.cfg
    ports:
      - "${HA_PROXY_BIND_PORT}:${HA_PROXY_BIND_PORT}"
    container_name: haproxy
    networks:
      demo-network:
        ipv4_address: ${HAPROXY_IP}
    depends_on:
      ha-client-controller-1:
        condition: service_healthy
    extra_hosts:
      - "${OC_URL}:${OC_IP}"
  browser:
    image: ${DOCKER_IMAGE_BROWSER_BOX}
    container_name: webtop
    security_opt:
      - seccomp:unconfined #optional
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
      - SUBFOLDER=/ #optional
      - TITLE=Webtop #optional
    volumes:
      - ${BROWSER_PERSISTENCE}:/config
    ports:
      - 3000:3000
      - 3001:3001
    #devices:
    #  - /dev/dri:/dev/dri #optional
    shm_size: "2gb" #optional
    restart: unless-stopped
    extra_hosts:
      - "${OC_URL}:${HAPROXY_IP}"
      - "${CLIENTS_URL}:${HAPROXY_IP}"
    networks:
      demo-network:
        ipv4_address: ${BROWSER_IP}
networks:
  demo-network:
    driver: bridge
    #driver_opts:
    #  com.docker.network.bridge.host_binding_ipv4: "127.0.0.1"
    ipam:
      config:
        - subnet: ${IP_PREFIX}.0.0/16
